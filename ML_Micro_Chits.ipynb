{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exp - 3\n"
      ],
      "metadata": {
        "id": "93mx9F-oelZR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "Wwgmsi6Adezm",
        "outputId": "093ed4d5-e8d6-4e42-ec6d-14515302a89f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'id3.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6b940ab1e792>\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id3.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mnode1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The decision tree for the dataset using ID3 algorithm is\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-6b940ab1e792>\u001b[0m in \u001b[0;36mload_csv\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'id3.csv'"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import csv\n",
        "\n",
        "def load_csv(filename):\n",
        "    lines = csv.reader(open(filename, \"r\"))\n",
        "    dataset = list(lines)\n",
        "    headers = dataset.pop(0)\n",
        "    return dataset, headers\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, attribute):\n",
        "        self.attribute = attribute\n",
        "        self.children = []\n",
        "        self.answer = \"\"\n",
        "\n",
        "def subtables(data, col, delete):\n",
        "    dic = {}\n",
        "    coldata = [row[col] for row in data]\n",
        "    attr = list(set(coldata))\n",
        "    counts = [0] * len(attr)\n",
        "    for x in range(len(attr)):\n",
        "        counts[x] = coldata.count(attr[x])\n",
        "        dic[attr[x]] = [row for row in data if row[col] == attr[x]]\n",
        "        if delete:\n",
        "            for row in dic[attr[x]]:\n",
        "                del row[col]\n",
        "    return attr, dic\n",
        "\n",
        "def entropy(S):\n",
        "    attr = list(set(S))\n",
        "    if len(attr) == 1:\n",
        "        return 0\n",
        "    counts = [S.count(a) / len(S) for a in attr]\n",
        "    return sum([-cnt * math.log(cnt, 2) for cnt in counts])\n",
        "\n",
        "def compute_gain(data, col):\n",
        "    attr, dic = subtables(data, col, delete=False)\n",
        "    total_size = len(data)\n",
        "    total_entropy = entropy([row[-1] for row in data])\n",
        "    attr_entropy = sum((len(dic[a]) / total_size) * entropy([row[-1] for row in dic[a]]) for a in attr)\n",
        "    return total_entropy - attr_entropy\n",
        "\n",
        "def build_tree(data, features):\n",
        "    lastcol = [row[-1] for row in data]\n",
        "    if len(set(lastcol)) == 1:\n",
        "        node = Node(\"\")\n",
        "        node.answer = lastcol[0]\n",
        "        return node\n",
        "    gains = [compute_gain(data, col) for col in range(len(features))]\n",
        "    split = gains.index(max(gains))\n",
        "    node = Node(features[split])\n",
        "    features = features[:split] + features[split + 1:]\n",
        "    attr, dic = subtables(data, split, delete=True)\n",
        "    for x in attr:\n",
        "        child = build_tree(dic[x], features)\n",
        "        node.children.append((x, child))\n",
        "    return node\n",
        "\n",
        "def print_tree(node, level=0):\n",
        "    if node.answer:\n",
        "        print(\" \" * level, node.answer)\n",
        "        return\n",
        "    print(\" \" * level, node.attribute)\n",
        "    for value, n in node.children:\n",
        "        print(\" \" * (level + 1), value)\n",
        "        print_tree(n, level + 2)\n",
        "\n",
        "def classify(node, x_test, features):\n",
        "    if node.answer:\n",
        "        print(node.answer)\n",
        "        return\n",
        "    pos = features.index(node.attribute)\n",
        "    for value, n in node.children:\n",
        "        if x_test[pos] == value:\n",
        "            classify(n, x_test, features)\n",
        "\n",
        "dataset, features = load_csv(\"id3.csv\")\n",
        "node1 = build_tree(dataset, features)\n",
        "print(\"The decision tree for the dataset using ID3 algorithm is\")\n",
        "print_tree(node1)\n",
        "\n",
        "testdata, features = load_csv(\"id3_test_1.csv\")\n",
        "for xtest in testdata:\n",
        "    print(\"The test instance:\", xtest)\n",
        "    print(\"The label for test instance:\", end=\" \")\n",
        "    classify(node1, xtest, features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exp - 5"
      ],
      "metadata": {
        "id": "TNezfZe5eoP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "import math\n",
        "\n",
        "def load_csv(filename):\n",
        "    lines = csv.reader(open(filename, \"r\"))\n",
        "    dataset = list(lines)\n",
        "    for i in range(len(dataset)):\n",
        "        dataset[i] = [float(x) for x in dataset[i]]\n",
        "    return dataset\n",
        "\n",
        "def split_dataset(dataset, split_ratio):\n",
        "    train_size = int(len(dataset) * split_ratio)\n",
        "    train_set = []\n",
        "    copy = list(dataset)\n",
        "    while len(train_set) < train_size:\n",
        "        index = random.randrange(len(copy))\n",
        "        train_set.append(copy.pop(index))\n",
        "    return [train_set, copy]\n",
        "\n",
        "def separate_by_class(dataset):\n",
        "    separated = {}\n",
        "    for i in range(len(dataset)):\n",
        "        vector = dataset[i]\n",
        "        if vector[-1] not in separated:\n",
        "            separated[vector[-1]] = []\n",
        "        separated[vector[-1]].append(vector)\n",
        "    return separated\n",
        "\n",
        "def mean(numbers):\n",
        "    return sum(numbers) / float(len(numbers))\n",
        "\n",
        "def stdev(numbers):\n",
        "    avg = mean(numbers)\n",
        "    variance = sum([pow(x - avg, 2) for x in numbers]) / float(len(numbers) - 1)\n",
        "    return math.sqrt(variance)\n",
        "\n",
        "def summarize(dataset):\n",
        "    summaries = [(mean(attribute), stdev(attribute)) for attribute in zip(*dataset)]\n",
        "    del summaries[-1]\n",
        "    return summaries\n",
        "\n",
        "def summarize_by_class(dataset):\n",
        "    separated = separate_by_class(dataset)\n",
        "    summaries = {}\n",
        "    for class_value, instances in separated.items():\n",
        "        summaries[class_value] = summarize(instances)\n",
        "    return summaries\n",
        "\n",
        "def calculate_probability(x, mean, stdev):\n",
        "    exponent = math.exp(-(math.pow(x - mean, 2) / (2 * math.pow(stdev, 2))))\n",
        "    return (1 / (math.sqrt(2 * math.pi) * stdev)) * exponent\n",
        "\n",
        "def calculate_class_probabilities(summaries, input_vector):\n",
        "    probabilities = {}\n",
        "    for class_value, class_summaries in summaries.items():\n",
        "        probabilities[class_value] = 1\n",
        "        for i in range(len(class_summaries)):\n",
        "            mean, stdev = class_summaries[i]\n",
        "            x = input_vector[i]\n",
        "            probabilities[class_value] *= calculate_probability(x, mean, stdev)\n",
        "    return probabilities\n",
        "\n",
        "def predict(summaries, input_vector):\n",
        "    probabilities = calculate_class_probabilities(summaries, input_vector)\n",
        "    best_label, best_prob = None, -1\n",
        "    for class_value, probability in probabilities.items():\n",
        "        if best_label is None or probability > best_prob:\n",
        "            best_prob = probability\n",
        "            best_label = class_value\n",
        "    return best_label\n",
        "\n",
        "def get_predictions(summaries, test_set):\n",
        "    predictions = []\n",
        "    for i in range(len(test_set)):\n",
        "        result = predict(summaries, test_set[i])\n",
        "        predictions.append(result)\n",
        "    return predictions\n",
        "\n",
        "def get_accuracy(test_set, predictions):\n",
        "    correct = 0\n",
        "    for i in range(len(test_set)):\n",
        "        if test_set[i][-1] == predictions[i]:\n",
        "            correct += 1\n",
        "    return (correct / float(len(test_set))) * 100.0\n",
        "\n",
        "def main():\n",
        "    filename = 'naivedata.csv'\n",
        "    split_ratio = 0.67\n",
        "    dataset = load_csv(filename)\n",
        "    training_set, test_set = split_dataset(dataset, split_ratio)\n",
        "    print(f'Split {len(dataset)} rows into train={len(training_set)} and test={len(test_set)} rows')\n",
        "\n",
        "    summaries = summarize_by_class(training_set)\n",
        "    predictions = get_predictions(summaries, test_set)\n",
        "    accuracy = get_accuracy(test_set, predictions)\n",
        "    print(f'Accuracy of the classifier is: {accuracy}%')\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "P5TSEjBhelBY",
        "outputId": "3d693a72-04a3-4d7c-b28a-c0cbc0c542ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'naivedata.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cbfb66cdbedb>\u001b[0m in \u001b[0;36m<cell line: 99>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Accuracy of the classifier is: {accuracy}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-cbfb66cdbedb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'naivedata.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0msplit_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.67\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Split {len(dataset)} rows into train={len(training_set)} and test={len(test_set)} rows'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-cbfb66cdbedb>\u001b[0m in \u001b[0;36mload_csv\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'naivedata.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exp - 1"
      ],
      "metadata": {
        "id": "cumRoMJXfTwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('enjoysport.csv')\n",
        "\n",
        "def total_training_instances(df):\n",
        "    return len(df)\n",
        "\n",
        "total_instances = total_training_instances(df)\n",
        "print(\"The total number of training instances are:\", total_instances)\n",
        "\n",
        "def initial_hypothesis(df):\n",
        "    return ['0'] * (len(df.columns) - 1)\n",
        "\n",
        "init_hypothesis = initial_hypothesis(df)\n",
        "print(\"The initial hypothesis is:\", init_hypothesis)\n",
        "\n",
        "def hypothesis_for_instances(df):\n",
        "    hypotheses = []\n",
        "    specific_hypothesis = initial_hypothesis(df)\n",
        "    for _, row in df.iterrows():\n",
        "        if row['enjoysport'] == 'yes':\n",
        "            for i in range(len(df.columns) - 1):\n",
        "                if specific_hypothesis[i] == '0':\n",
        "                    specific_hypothesis[i] = row[i]\n",
        "                elif specific_hypothesis[i] != row[i]:\n",
        "                    specific_hypothesis[i] = '?'\n",
        "            hypotheses.append(specific_hypothesis.copy())\n",
        "    return hypotheses\n",
        "\n",
        "hypotheses = hypothesis_for_instances(df)\n",
        "for idx, h in enumerate(hypotheses, start=1):\n",
        "    print(f\"The hypothesis for the training instance {idx} is: {h}\")\n",
        "\n",
        "def find_s_algorithm(df):\n",
        "    specific_hypothesis = initial_hypothesis(df)\n",
        "    for _, row in df.iterrows():\n",
        "        if row['enjoysport'] == 'yes':\n",
        "            for i in range(len(df.columns) - 1):\n",
        "                if specific_hypothesis[i] == '0':\n",
        "                    specific_hypothesis[i] = row[i]\n",
        "                elif specific_hypothesis[i] != row[i]:\n",
        "                    specific_hypothesis[i] = '?'\n",
        "    return specific_hypothesis\n",
        "\n",
        "max_specific_hypothesis = find_s_algorithm(df)\n",
        "print(\"The Maximally specific hypothesis for the training instances is:\", max_specific_hypothesis)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "t-vbVnddemma",
        "outputId": "e5d31122-bd02-41ad-ab22-50eb5147a258"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'enjoysport.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a1d11bf4c4be>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'enjoysport.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtotal_training_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'enjoysport.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-HO0zmbGf9pr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}